{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Notizen & offene Fragen sind immer in rot eingefügt\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and comparison of K-nearest neighbors (KNN) and convolutional neural network (CNN) for clothes recognition\n",
    "### Data Analysis MoBi SoSe2023, Topic 01: Image Analysis\n",
    "### Tutor: Hannah Winter\n",
    "### Team 04: Ole Decker, Heinrike Gilles, Bastian Mucha, Anastasia Warken\n",
    "#### July 2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "<span style=\"color:red\">\n",
    "- What's KNN, what's CNN <br>\n",
    "- Whats the goal of our project? <br>\n",
    "- What is our extra work (zB KD Trees, special CNN for shirts and T-Shirts) <br>\n",
    "- Main result: KNN worked suprisingly good compared to CNN <br>\n",
    "</span>\n",
    "\n",
    "K-nearest neighbors is a classical machine learning algorithm that was first introduced in \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "### Z-transformation\n",
    "A Z-transformation is a statistical technique used to standardize and normalize data. It transforms a dataset into a standard normal distribution, which has a mean of 0 and a standard deviation of 1. It is performed by subtracting the mean of the dataset from each datapoint and then dividing it by the standard deviation. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "here insertion of the Z-transformation formula??\n",
    "</span>\n",
    "\n",
    "Regarding the MNIST fashion dataset the Z-Transformation is applied to the intensity value of each pixel of each image. This is done to improve the performance and convergence of our KNN and CNN. Standardizing the pixel intensities helps to avoid that certain features dominate the learning process, e.g. high intensity pixels in the background of the images. Normalizing the data makes the pixels comparable to one another. \n",
    "\n",
    "### PCA\n",
    "A Principal Component Analysis (PCA) is a method used to reduce the dimensions of a high dimensional dataset while preserving the most important information and minimizing the loss of variance. This technique is done as a preparation to make a machine learnig process more efficient by reducing redundancy.\n",
    "To perform a PCA a Covariance matrix is calculated using the Z-transformed dataset. The covariance matrix gives the relationship of each pixel intensity value of all images. The columns of the covariance matrix are the eigenvectors. The eigenvalues are calculated using the covariance matrix. Each eigenvalue corresponds to an eigenvector and represents the explained variance of this vector. The larger the eigenvalue, the more variance is captured along that eigenvector or principal component. <br>\n",
    "<span style=\"color:red\">\n",
    "hier Gleichung zur Berechnung von Covarianzmatrix & Berechnung für Eigenvalues einfügen? <br>\n",
    "</span>\n",
    "The final step in a PCA is to choose the percentage of variance the data is supposed to describe and then removing all redundant Principal Components.\n",
    "Our team coded a PCA using NumPy by computing a covariance matrix using the Z-transformed data and then calculating the eigenvalues. <br>\n",
    "These were then resorted in ascending order by absolute value, meaning the algebraic sign was removed since eigenvalues can be positive or negative., but only the absolute value gives information about how much variance an eigenvalue covers.  \n",
    "\n",
    "### K-nearest neighbors (KNN)\n",
    "K-nearest neighbors (KNN) is a non-parametric, supervised learning algorithm. In the context of our project it was used for classification but it can also be used for regression tasks. The KNN makes predictions based on the similarity of input data points to their neighboring data points by measuring the distance to all reference points and then finding the k nearest points. As distance calculation method, we chose the euclidian distance. The class of the test data point is assigned by a so-called majority vote, meaning the class the majority of the nearest points have is selected. \n",
    "\n",
    "KNN belongs to the family of lazy-learning models. This implies that the algorithm stores the entire training dataset and uses it as reference for each testing data point instead of undergoing a training phase. This approach has consequences on the run time.\n",
    "To calculate the k nearest neighbors the size of k has to be determined. To find the optimal k we used the proof by exhaustion method. We let the KNN run with different numbers for k \n",
    "<span style=\"color:red\"> \n",
    "and then measured the performance and chose the optimal value for k with the highest accuracy. Normally this method is done using a validation data set, but we directly used the test data set. <br>\n",
    "</span>\n",
    "\n",
    "The aim for our KNN was to correctly classify images of clothing items, by comparing each test image with all images from the training data set and then comparing the predicted labels to the actual classification. The closest neighbors of the sample image were determined based on the smallest difference in intensity values between the images.\n",
    "<span style=\"color:red\"> \n",
    "In our code we defined the function \"dist\" to .....\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\"> \n",
    "- Vllt proof by exhaustion noch genauer erklären <br>\n",
    "- Einzelnen Steps unserer gecodeten KNN erklären\n",
    "</span>\n",
    "\n",
    "### KD-trees\n",
    "Since the KNN algorithm is not very time efficient due to it's way of operating, k-dimensioan trees or KD-trees are a tool to use clever data structures to optimize the classification process. K is defined by the number of properties each data point has, in this context it is the dimensions, so it corresponds to the number of PCs selected.\n",
    "The concept is based on the repeated division of the space along the median value of one of the dimensions. This can be shown in a binary tree structure. Each node is a splitting hyperplane, which is defined by one of the axis. \n",
    "<span style=\"color:red\"> \n",
    "The points are divided in the tree whether the coordinate value for that axis is bigger or smaller.\n",
    "</span>\n",
    "In which order the axes are chosen during division can vary, but the most common approach is to rotate through each of the dimensions.\n",
    "How many partitions of the space are made can be chosen by changing the leaf size. The leaf size is the maximal number of data points that can be contained in a final node at the bottom of the KD-tree. A smaller leaf size generates a larger tree with more partitions, making the construction time longer, a larger leaf size can result in an unbalanced tree and slow down the search time. However the leaf size does not affect the result of the query.\n",
    "<span style=\"color:red\"> \n",
    "- hier KD Tree einfügen\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> \n",
    " - Pathfinding along tree to locate data point, go up on tree to locate any data points <br>\n",
    " -'hiding' in neighboring spaces <br>\n",
    " - Working uo the tree until the distance between the division axes exceeds distance of data point w/ smallest distance <br>\n",
    " </span>\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "<span style=\"color:red\"> \n",
    "- deep learning algorithm\n",
    "- node layers\n",
    "    - input layer\n",
    "    - hidden layer(s)\n",
    "    - threshold\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
