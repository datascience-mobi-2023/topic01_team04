{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Notizen & offene Fragen sind immer in rot eingef√ºgt\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and comparison of K-nearest neighbors (KNN) and convolutional neural network (CNN) for clothes recognition\n",
    "### Data Analysis MoBi SoSe2023, Topic 01: Image Analysis\n",
    "### Tutor: Hannah Winter\n",
    "### Team 04: Ole Decker, Heinrike Gilles, Bastian Mucha, Anastasia Warken\n",
    "#### July 2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "<span style=\"color:red\">\n",
    "- What's KNN, what's CNN <br>\n",
    "- Whats the goal of our project? <br>\n",
    "- What is our extra work (zB KD Trees, special CNN for shirts and T-Shirts) <br>\n",
    "- Main result: KNN worked suprisingly good compared to CNN <br>\n",
    "</span>\n",
    "\n",
    "K-nearest neighbors is a classical machine learning algorithm that was first introduced in \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "### Z-transformation\n",
    "A Z-transformation is a statistical technique used to standardize and normalize data. It transforms a dataset into a standard normal distribution, which has a mean of 0 and a standard deviation of 1. It is performed by subtracting the mean of the dataset from each datapoint and then dividing it by the standard deviation. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "here insertion of the Z-transformation formula??\n",
    "</span>\n",
    "\n",
    "Regarding the MNIST fashion dataset the Z-Transformation is applied to the intensity value of each pixel of each image. This is done to improve the performance and convergence of our KNN and CNN. Standardizing the pixel intensities helps to avoid that certain features dominate the learning process, e.g. high intensity pixels in the background of the images. Normalizing the data makes the pixels comparable to one another. \n",
    "\n",
    "### PCA\n",
    "A Principal Component Analysis (PCA) is a method used to reduce the dimensions of a high dimensional dataset while preserving the most important information and minimizing the loss of variance. This technique is done as a preparation to make a machine learnig process more efficient by reducing redundancy.\n",
    "To perform a PCA a Covariance matrix is calculated using the Z-transformed dataset. The covariance matrix gives the relationship of each pixel intensity value of all images. The columns of the covariance matrix are the eigenvectors. The eigenvalues are calculated using the covariance matrix. Each eigenvalue corresponds to an eigenvector and represents the explained variance of this vector. The larger the eigenvalue, the more variance is captured along that eigenvector or principal component. <br>\n",
    "<span style=\"color:red\">\n",
    "-> Graphically: Diagonalization translates to a rotation of correlation matrix, base vectors being PCs <br>\n",
    "</span>\n",
    "The final step in a PCA is to choose the percentage of variance the data is supposed to describe and then removing all redundant Principal Components.\n",
    "Our team coded a PCA using NumPy by computing a covariance matrix using the Z-transformed data and then calculating the eigenvalues. <br>\n",
    "<span style=\"color:red\">\n",
    "These were then resorted in ascending order by absolute value, meaning the algebraic sign was removed since a negative eigenvalue explains more variance than an eigenvalue close to zero. \n",
    "</span>\n",
    "\n",
    "###KNN\n",
    "non-parametric, supervised learning algorithm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
